{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nehemiahloury/anaconda/envs/dsi/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pickle as pic\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import nltk, string\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "all_data = pic.load(open(\"../Data/all_data.p\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73933\n",
      "35603\n"
     ]
    }
   ],
   "source": [
    "# Get Text and Index\n",
    "ann_text = all_data['ann_text'].values\n",
    "ref_text = all_data['fragment'].values\n",
    "\n",
    "# Define normalizer\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens if item not in stop]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(\n",
    "            text.lower().translate(None, string.punctuation)))\n",
    "\n",
    "# Normalize our annotations and Referent Text\n",
    "ann_text_bow = np.array(map(normalize, ann_text))\n",
    "ref_text_bow = np.array(map(normalize, ref_text))\n",
    "\n",
    "ann_dictionary = corpora.Dictionary(ann_text_bow)\n",
    "ref_dictionary = corpora.Dictionary(ref_text_bow)\n",
    "\n",
    "# Print Number of Words in the Dictionary\n",
    "print len(ann_dictionary.values())\n",
    "print len(ref_dictionary.values())\n",
    "\n",
    "\n",
    "# Create our corpuses\n",
    "ann_corpus_bow = np.array([ann_dictionary.doc2bow(text) for text in ann_text_bow])\n",
    "ref_corpus_bow = np.array([ann_dictionary.doc2bow(text) for text in ref_text_bow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Annotation LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics being evaluated: 10\n",
      "Model trained in 172.823943853 seconds.\n",
      "Perplexity found in 142.275618076 seconds.\n",
      "Coherence scores found in 63.4813780785 seconds.\n",
      "[-7033183.6672986522, 820.46647954697676, 0.55857028354711791, -2.3726795047188407]\n",
      "Number of topics being evaluated: 20\n",
      "Model trained in 181.846566916 seconds.\n",
      "Perplexity found in 143.11157012 seconds.\n",
      "Coherence scores found in 99.7857139111 seconds.\n",
      "[-7304996.6740683373, 1063.3621488539711, 0.55282878328002494, -2.4959181051983959]\n",
      "Number of topics being evaluated: 30\n",
      "Model trained in 193.702538013 seconds.\n",
      "Perplexity found in 150.282443047 seconds.\n",
      "Coherence scores found in 158.997165918 seconds.\n",
      "[-7554470.3893681429, 1349.1048620147867, 0.56092496928770297, -2.6135371563231673]\n",
      "Number of topics being evaluated: 40\n",
      "Model trained in 206.005049944 seconds.\n",
      "Perplexity found in 127.240170002 seconds.\n",
      "Coherence scores found in 195.903263092 seconds.\n",
      "[-7782340.1108094342, 1676.714149076671, 0.54172728302812756, -3.106572452005635]\n",
      "Number of topics being evaluated: 50\n",
      "Model trained in 176.834993124 seconds.\n",
      "Perplexity found in 155.849294901 seconds.\n",
      "Coherence scores found in 278.570642948 seconds.\n",
      "[-8019778.6907468066, 2102.9890848896257, 0.52735401836540474, -3.2531415493257159]\n",
      "Number of topics being evaluated: 60\n",
      "Model trained in 209.961028814 seconds.\n",
      "Perplexity found in 163.278723001 seconds.\n",
      "Coherence scores found in 775.52936697 seconds.\n",
      "[-8240668.8564572036, 2596.3216759311681, 0.51216529297684454, -3.4579363578917959]\n",
      "Number of topics being evaluated: 70\n",
      "Model trained in 555.119144917 seconds.\n",
      "Perplexity found in 333.862808943 seconds.\n",
      "Coherence scores found in 447.364156961 seconds.\n",
      "[-8463382.7150656078, 3210.9651417540445, 0.51608629022004293, -3.6384382054876747]\n",
      "Number of topics being evaluated: 80\n",
      "Model trained in 287.712259054 seconds.\n",
      "Perplexity found in 164.799166918 seconds.\n",
      "Coherence scores found in 621.945026875 seconds.\n",
      "[-8663301.6189059839, 3885.6890456694478, 0.4844798915424976, -4.0731459678773678]\n",
      "Number of topics being evaluated: 90\n",
      "Model trained in 306.469048977 seconds.\n",
      "Perplexity found in 174.231701136 seconds.\n",
      "Coherence scores found in 634.692445993 seconds.\n",
      "[-8883611.1149970051, 4794.5619512793292, 0.47464960213809793, -4.3281244219290791]\n",
      "Number of topics being evaluated: 100\n",
      "Model trained in 217.828696012 seconds.\n",
      "Perplexity found in 142.988425016 seconds.\n",
      "Coherence scores found in 603.92327714 seconds.\n",
      "[-9084735.155159656, 5808.7235930790976, 0.46002848503689187, -4.656612389609581]\n"
     ]
    }
   ],
   "source": [
    "topic_num_list = range(10, 101 ,10)\n",
    "ann_results = test_topic_num(topic_num_list, ann_corpus_bow, ann_text_bow, ann_dictionary, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ann_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Referent LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ref_results = test_topic_num(topic_num_list, ref_corpus_bow, ref_text_bow, ref_dictionary, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def lda_evaluation(corpus, texts, dictionary, num_folds=3, topics=10, random_state=None):\n",
    "    # Create train-test split\n",
    "    train_corp, test_corp, train_texts, test_texts = train_test_split(corpus, texts, random_state=random_state)\n",
    "\n",
    "    # Train LDA Model\n",
    "    start = time.time()\n",
    "    lda = models.LdaMulticore(train_corp, id2word=dictionary, \n",
    "                              num_topics=topics, iterations=50)\n",
    "    train_time = time.time()\n",
    "    print \"Model trained in %s seconds.\" % (train_time - start)\n",
    "    \n",
    "    # Find perplexity on held out documents\n",
    "    perplexity = lda.bound(test_corp)\n",
    "    number_of_words = sum(len(doc) for doc in test_texts)\n",
    "    per_word_perplex = np.exp2(-perplexity / number_of_words)\n",
    "    perplexity_time = time.time()\n",
    "    print \"Perplexity found in %s seconds.\" % (perplexity_time - train_time)\n",
    "    \n",
    "    # Calculate coherence measures\n",
    "    cm_cv = CoherenceModel(model=lda, texts=train_texts, \n",
    "                           dictionary=dictionary, coherence='c_v')\n",
    "    cm_umass = CoherenceModel(model=lda, corpus=train_corp, \n",
    "                              dictionary=dictionary, coherence='u_mass')\n",
    "    cv_coherence = cm_cv.get_coherence()\n",
    "    umass_coherence = cm_umass.get_coherence()\n",
    "    coherence_time = time.time()\n",
    "    print \"Coherence scores found in %s seconds.\" % (coherence_time - perplexity_time)\n",
    "    \n",
    "    # Return results\n",
    "    result = [perplexity, per_word_perplex, cv_coherence, umass_coherence]\n",
    "    return result\n",
    "\n",
    "def test_topic_num(topic_num_list, corpus, texts, dictionary, random_state=None):\n",
    "    # Create Reults DataFrame\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # Evaluate topic model for each number of topics in list\n",
    "    for num in topic_num_list:\n",
    "        print \"Number of topics being evaluated: %s\" % num\n",
    "        evaluation = lda_evaluation(corpus, texts, dictionary, \n",
    "                                    topics=num, random_state=random_state)\n",
    "        print evaluation\n",
    "        scores = pd.DataFrame(np.array([num] + evaluation, ndmin=2))\n",
    "        results = results.append(scores)\n",
    "    \n",
    "    results.rename(columns={0: 'num_topics', 1: 'perplex', 2: 'per_word_perplex', \n",
    "                            3: 'cv', 4: 'umass'}, inplace=True)    \n",
    "    return results\n",
    "\n",
    "def best_topic_num(results):\n",
    "    results_sorted = sorted(results.items(), key=lambda x: x[1][1])\n",
    "    return results_sorted[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
